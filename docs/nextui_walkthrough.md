# Deep Dive: Next.js UI Integration

This document serves as a comprehensive technical guide to the Next.js UI integration. It details the transition from a prototype Streamlit interface to a robust, production-ready React application, explaining the "what", "how", and "why" of each architectural decision.

## 1. Technology Overview

Before diving into the implementation, it is crucial to understand the core technologies selected for the new frontend stack.

### React

[React](https://react.dev/) is a JavaScript library for building user interfaces based on components.

*   **Why we chose it**: React's component-based architecture allows us to build encapsulated UI elements (like the Chat bubble, the Personality selector) that manage their own state. It also boasts a massive ecosystem and community support.
*   **Key Concept**: **Virtual DOM**. React maintains a lightweight representation of the UI in memory. When state changes (e.g. a new message arrives), React compares the virtual DOM with the real DOM and efficiently updates only the changed elements.

### Next.js

[Next.js](https://nextjs.org/) is an opinionated meta-framework built on top of React. It handles the plumbing that React itself doesn't provide out of the box.

*   **Why we chose it**: It provides a production-ready environment with features like:
    *   **File-system based routing**: Creating a file in `pages/` automatically creates a route.
    *   **Fast Refresh**: Instant feedback during development.
    *   **Built-in Optimization**: Automatic image optimization and code splitting.
*   **Configuration**: We are using **Next.js** with the **Pages Router** (classic routing) for stability and compatibility with our current deployment setup.

### Material UI (MUI)

[MUI](https://mui.com/) is a comprehensive library of UI components implementing Google's [Material Design](https://m3.material.io/) system.

*   **Why we chose it**:
    *   **Velocity**: It provides pre-built, accessible components (Inputs, Buttons, Avatars) so we don't have to build them from scratch.
    *   **Aesthetics**: It offers a polished, professional look out-of-the-box that aligns with the "Google-y" nature of this project.
    *   **Theming**: Its `sx` prop allows for powerful, inline CSS-in-JS styling without the mess of global CSS files.

---

## 2. Implementation Walkthrough

The implementation of the new UI required changes across the entire stack.

### Phase 1: The Backend (API Layer)

**Goal**: Transform the API from a "dumb pipe" to a "single source of truth".

#### Step 1A: Dynamic Configuration Endpoint

We created a new API endpoint to expose the backend's persona configuration to the frontend.

*   **Implementation**:
    1.  Defined a Pydantic model `Persona` in `src/main.py` to enforce the data structure (name, description, avatar).
    2.  Implemented `GET /personas` which calls `rickbot_agent.personality.get_personalities()`.
    3.  This endpoint returns a JSON list of all active agents defined in `personalities.yaml`.

**Rationale**: This decouples the frontend from the backend configuration. The UI becomes "data-driven"â€”it simply renders whatever the backend tells it is available.

#### Step 1B: Refined Streaming Logic

Large Language Models (LLMs) take time to generate full responses. Waiting 5+ seconds for a complete answer feels unresponsive to the user. Real-time token streaming using Server-Sent Events (SSE) provides a much better user experience.

*   **Implementation**:
    *   The `event_generator` function in `src/main.py` yields data in a precise format: `data: {"chunk": "..."}\n\n`.
    *   We specifically emit `session_id` in the first chunk to ensure the stateless frontend can maintain conversation continuity.

**Rationale**: SSE is a standard HTTP protocol perfect for one-way streams (Server -> Client). It is simpler to implement than WebSockets for this use case and natively supported by browsers.

---

### Phase 2: The Frontend (Next.js Application)

**Goal**: Build a responsive, stateful chat interface.

#### Step 2A: Project Structure

We initialized the app in `src/nextjs_fe`.
*   `pages/index.tsx`: The main entry point.
*   `components/Chat.tsx`: The monolithic Chat component containing the core logic.
*   `public/avatars/`: Static assets for the character images.

#### Step 2B: State Management (`Chat.tsx`)

We use React's `useState` hook to track the application's vital signs:
*   `messages`: An array of message objects (both User and Bot).
*   `personalities`: The list of available agents (fetched from API).
*   `selectedPersonality`: The currently active persona (e.g., "Rick").
*   `streamingText`: A temporary buffer for the message currently being generated.

**Rationale**: By keeping `streamingText` separate from the committed `messages` array, we can render the "typing" effect smoothly without constantly re-creating the entire message list.

#### Step 2C: Dynamic Personas Integration

**Implementation**:
We used the `useEffect` hook to trigger a "side effect" when the component first loads.
```typescript
useEffect(() => {
    const fetchPersonalities = async () => {
        const response = await axios.get('/personas');
        setPersonalities(response.data); // Update state with backend data
    };
    fetchPersonalities();
}, []); // Empty dependency array = run once on mount
```

**Rationale**: This ensures that as soon as the user opens the page, they see the most up-to-date list of characters without manual hardcoding.

#### Step 2D: The Streaming Consumer

**The Challenge**: Browsers' default `fetch` API creates a Promise that resolves only when the *entire* response is complete. We need to access the data *as it arrives*.

**Implementation**:
1.  We use the low-level `response.body.getReader()` API.
2.  We enter a `while(true)` loop that reads chunks of raw bytes.
3.  We pass these bytes through a `TextDecoder` to get strings.
4.  We parse the custom SSE format (`data: {...}`) to extract text tokens, **session IDs**, and **tool-call events**.
5.  We append tokens to `streamingText`, causing React to re-render the UI instantly.

#### Step 2E: Multimodal Support (File Uploads)

**Goal**: Allow users to upload documents (PDF, CSV, Images) for RAG or visual analysis.

*   **Implementation**:
    1.  Uses a hidden `<input type="file" multiple />` triggered by a custom MUI `IconButton`.
    2.  Files are stored in the local `files` state and displayed as chip-like previews.
    3.  During `handleSendMessage`, we use a `FormData` object to send the prompt and file blobs to the backend as a `multipart/form-data` request.
    4.  The API response is still streamed via SSE, allowing the UI to show progress while the files are being processed.

---

## 3. Advanced Features & UI/UX Patterns

### 3.1 Role-Based Access Control (RBAC) & Upgrades

The system enforces persona restrictions directly in the UI. 
*   **UPGRADE_REQUIRED**: If the backend returns a `403` with a specific `UPGRADE_REQUIRED` error code, the `Chat` component catches this and opens a themed **Upgrade Required Modal**.
*   **Immersion**: The modal encourages users to "become a Supporter" to unlock restricted interdimensional personalities.

### 3.2 Thinking & Tool Call Visualisation

To bridge the gap between "Sent" and "Responding", the UI uses a dedicated `Thinking` component.
*   **Stateful Feedback**: Uses the `botAction` and `activeTool` states to show what the agent is currently doing (e.g., "Using tool: GoogleSearch...", "Transferring to agent: RagAgent...").
*   **Visual Continuity**: This ensures the user feels the system is active even during complex, multi-step agent operations like RAG retrieval.

### 3.3 Portals & Immersion

*   **Warm-up Screen**: To handle backend "Cold Starts" (loading heavy ML models), we implemented a "Heating up the portal gun..." screen with a pulsing animation.
*   **Portal Animations**: The `PortalAnimation` component triggers a swirling green portal effect whenever a message is sent, maintaining the Rick & Morty aesthetic.
*   **Dark Mode**: We implemented a dark theme (`ThemeProvider`) with a galaxy background to match the sci-fi tone.

