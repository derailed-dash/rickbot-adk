{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93b1ec65",
   "metadata": {},
   "source": [
    "# File Search Store Management for Rickbot\n",
    "\n",
    "A notebook to experiment with the FileSearchStore and how it can be used to manage file search in the Rickbot Agent.\n",
    "\n",
    "The best way to run this notebook is from Google Colab.\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/derailed-dash/rickbot-adk/blob/main/notebooks/file_search_store.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=30/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a639f75a",
   "metadata": {},
   "source": [
    "## Pre-Reqs and Notes\n",
    "\n",
    "- The `file_search_stores` is a feature exclusive to the Gemini Developer API. \n",
    "  - It does not work with the Vertex AI API or the Gen AI SDK in Vertex AI mode.\n",
    "  - Therefore: don't set env vars for `GOOGLE_CLOUD_LOCATION` or `GOOGLE_GENAI_USE_VERTEXAI` and do not initialise Vertex AI.\n",
    "- Make sure you have an up-to-date version of the `google-genai` package installed. \n",
    "  - Versions older than 1.49.0 do not support the File Search Tool.\n",
    "  - You can upgrade all packages using `uv sync --upgrade`.\n",
    "  - Or just `google-genai` using `uv sync --upgrade-package google-genai`\n",
    "  - Or, if using `pip`: `pip install --upgrade google-genai`.\n",
    "  - You can add to your `pyproject.toml` file; since we don't explicitly need it outside \n",
    "- Add your Gemini API Key to Colab as a secret. Then you can retrieve it using `userdata.get(\"GEMINI_API_KEY\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d68e9c1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97210f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e576bcec",
   "metadata": {},
   "source": [
    "\n",
    "### Local Only\n",
    "\n",
    "If running locally, setup the Google Cloud environment:\n",
    "\n",
    "```bash\n",
    "source scripts/setup-env.sh\n",
    "```\n",
    "\n",
    "Then to install the package dependencies into the virtual environment, use the `uv` tool:\n",
    "\n",
    "1. From your agent's root directory, run `make install` to set up the virtual environment (`.venv`).\n",
    "2. In this Jupyter notebook, select the kernel from the `.venv` folder to ensure all dependencies are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a1ade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load env vars\n",
    "if load_dotenv():\n",
    "    print(\"Successfully loaded environment variables.\")\n",
    "else:\n",
    "    print(\"Failed to load environment variables.\")\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not GEMINI_API_KEY:\n",
    "    raise ValueError(\"GEMINI_API_KEY environment variable not set.\")\n",
    "else:\n",
    "    print(\"Successfully loaded Gemini API key.\")\n",
    "\n",
    "MODEL = os.getenv(\"MODEL\")\n",
    "if not MODEL:\n",
    "    print(\"Warning: MODEL environment variable not set.\")\n",
    "else:\n",
    "    print(f\"Successfully loaded model: {MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9a6f7f",
   "metadata": {},
   "source": [
    "### Or In Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ee8b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -U \"google-genai>=1.49.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bb10c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "\n",
    "os.environ[\"GEMINI_API_KEY\"] = userdata.get(\"GEMINI_API_KEY\")\n",
    "os.environ[\"MODEL\"] = userdata.get(\"MODEL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a074a3",
   "metadata": {},
   "source": [
    "### Client Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5175e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client()\n",
    "STORE_NAME = \"rickbot-dazbo-ref\" # as per personalities.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42055dd4",
   "metadata": {},
   "source": [
    "## Store Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c9c175",
   "metadata": {},
   "source": [
    "### View All Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1ec353b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='fileSearchStores/demofilestore-5fayifsabstz' display_name='demo-file-store' create_time=datetime.datetime(2026, 1, 9, 22, 0, 19, 996699, tzinfo=TzInfo(0)) update_time=datetime.datetime(2026, 1, 9, 22, 0, 19, 996699, tzinfo=TzInfo(0)) active_documents_count=1 pending_documents_count=None failed_documents_count=None size_bytes=5948\n",
      "name='fileSearchStores/rickbotdazboref-kw1ir7goyfuq' display_name='rickbot-dazbo-ref' create_time=datetime.datetime(2026, 1, 9, 23, 41, 30, 121403, tzinfo=TzInfo(0)) update_time=datetime.datetime(2026, 1, 9, 23, 41, 30, 121403, tzinfo=TzInfo(0)) active_documents_count=1 pending_documents_count=None failed_documents_count=None size_bytes=7422743\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for a_store in client.file_search_stores.list():\n",
    "        print(a_store)\n",
    "except Exception as e:\n",
    "    print(f\"Error listing stores (check creds?): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f0dfbd",
   "metadata": {},
   "source": [
    "### Retrieve the Store\n",
    "\n",
    "Here's a utility function to retrieve the store(s) that match a given display name. Note that display name is not unique, so this function returns the first matching store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b80190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_store(store_name: str):\n",
    "    \"\"\"Retrieve a store by its display name\"\"\"\n",
    "    try:\n",
    "        for a_store in client.file_search_stores.list():\n",
    "            if a_store.display_name == store_name:\n",
    "                return a_store\n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_store path: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d3346a",
   "metadata": {},
   "source": [
    "### Create the Store (One Time)\n",
    "\n",
    "Once you've created the store, save the store ID for use in your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6954029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not get_store(STORE_NAME):\n",
    "    file_search_store = client.file_search_stores.create(config={\"display_name\": STORE_NAME})\n",
    "    print(f\"Created store: {file_search_store.name}\")\n",
    "else:\n",
    "    print(f\"Store {STORE_NAME} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebe601a",
   "metadata": {},
   "source": [
    "### View the Store\n",
    "\n",
    "We can interrogate a store and see what files have been uploaded to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc694cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='fileSearchStores/rickbotdazboref-kw1ir7goyfuq' display_name='rickbot-dazbo-ref' create_time=datetime.datetime(2026, 1, 9, 23, 41, 30, 121403, tzinfo=TzInfo(0)) update_time=datetime.datetime(2026, 1, 9, 23, 41, 30, 121403, tzinfo=TzInfo(0)) active_documents_count=1 pending_documents_count=None failed_documents_count=None size_bytes=7422743\n",
      "Docs in rickbot-dazbo-ref: 1\n",
      "-----------\n",
      "Document 0:\n",
      "-----------\n",
      "  Display name:Sequencing Cloud Migration to Reduce Cost: What to Migrate and When\n",
      "  ID: fileSearchStores/rickbotdazboref-kw1ir7goyfuq/documents/sequencing-cloud-migration--bnrw4u8z5bf4\n",
      "  Metadata: [CustomMetadata(\n",
      "  key='title',\n",
      "  string_value='Sequencing Cloud Migration to Reduce Cost: What to Migrate and When'\n",
      "), CustomMetadata(\n",
      "  key='file_name',\n",
      "  string_value='8c - Sequencing Cloud Migration to Reduce Cost_ What to Migrate and When _ by Dazbo (Darren Lester) _ Google Cloud - Community _ Mar, 2025 _ Medium.pdf'\n",
      "), CustomMetadata(\n",
      "  key='author',\n",
      "  string_value='Dazbo (Darren Lester)'\n",
      "), CustomMetadata(\n",
      "  key='abstract',\n",
      "  string_value='How to reduce TCO and accelerate ROI by strategically sequencing cloud migrations based on contract expiry and platform dependency.'\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "file_search_store = get_store(STORE_NAME)\n",
    "if not file_search_store:\n",
    "    print(f\"Store {STORE_NAME} not found.\")\n",
    "else:\n",
    "    print(file_search_store)\n",
    "\n",
    "    # List all documents in the store\n",
    "    # The 'parent' argument is the resource name of the store\n",
    "    docs = client.file_search_stores.documents.list(parent=file_search_store.name)\n",
    "    try:\n",
    "        doc_list = list(docs)\n",
    "        print(f\"Docs in {STORE_NAME}: {len(doc_list)}\")\n",
    "\n",
    "        if not doc_list:\n",
    "            print(\"No documents found in the store.\")\n",
    "        else:\n",
    "            for i, doc in enumerate(doc_list):\n",
    "                section_heading = f\"Document {i}:\"\n",
    "                print(\"-\" * len(section_heading))\n",
    "                print(section_heading)\n",
    "                print(\"-\" * len(section_heading))\n",
    "                print(f\"  Display name:{doc.display_name}\")\n",
    "                print(f\"  ID: {doc.name}\")\n",
    "                print(f\"  Metadata: {doc.custom_metadata}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing docs (might be empty): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c82077a",
   "metadata": {},
   "source": [
    "### Delete Store(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d6987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, point to the right store. For example:\n",
    "store_to_delete = get_store(\"rickbot-adk-file-search-store\")\n",
    "\n",
    "# Delete the store\n",
    "if store_to_delete:\n",
    "    print(f\"Deleting: {store_to_delete.name}\")\n",
    "    # Uncomment to delete\n",
    "    # client.file_search_stores.delete(name=store_to_delete.name, config={'force': True})\n",
    "else:\n",
    "    print(\"Store not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11668f60",
   "metadata": {},
   "source": [
    "## Upload and Process Files\n",
    "\n",
    "Now we need to place the files in a suitable local folder to upload to the store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad4e1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPLOAD_PATH = \"/content/upload-files/\"\n",
    "UPLOAD_PATH = \"../scratch/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b5a9d7",
   "metadata": {},
   "source": [
    "Create some utility classes and functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6f54dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentMetadata(BaseModel):\n",
    "    \"\"\"Metadata for a document\"\"\"    \n",
    "    title: str\n",
    "    author: str\n",
    "    abstract: str\n",
    "\n",
    "def delete_doc(doc):\n",
    "    \"\"\"\n",
    "    Delete document(s) from its file search store.\n",
    "    Note that the doc already references its file search store.\n",
    "    So we don't need to pass the file search store name.\n",
    "    \"\"\"\n",
    "    print(f\"♻️  Deleting duplicate: '{doc.display_name}' (ID: {doc.name})\")\n",
    "    client.file_search_stores.documents.delete(name=doc.name, config={\"force\": True})\n",
    "    time.sleep(2)  # small throttle and allow propagation\n",
    "\n",
    "def generate_metadata(file_name: str, temp_file) -> DocumentMetadata:\n",
    "    \"\"\"Generate metadata for a document\"\"\"\n",
    "\n",
    "    print(f\"Extracting metadata from {file_name}...\")\n",
    "    response = client.models.generate_content(\n",
    "        model=MODEL,\n",
    "        contents=[\n",
    "            \"\"\"Please extract title, author, and short abstract from this document. \n",
    "            Each value should be under 200 characters.\n",
    "\n",
    "            Abstracts should be succinct and NOT include preamble text like `This document describes...`\n",
    "\n",
    "            Example bad abstract: \n",
    "            Now I want to cover a key consideration that can potentially \n",
    "            save you more in future IT spend than any other decision you can make: \n",
    "            embracing open source as a core element of your cloud strategy.\n",
    "\n",
    "            Example good abstract:\n",
    "            How you can significantly reduce IT spend by embracing open source\n",
    "            as a core component of your cloud strategy.\n",
    "\n",
    "            Example bad abstract:\n",
    "            This article discusses how you can design your cloud landing zone.\n",
    "\n",
    "            Example good abstract:\n",
    "            How to design your cloud landing zone according to best practices.\n",
    "            \"\"\",\n",
    "            temp_file,\n",
    "        ],\n",
    "        config={\n",
    "            \"response_mime_type\": \"application/json\",\n",
    "            \"response_schema\": DocumentMetadata,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    metadata: DocumentMetadata = response.parsed\n",
    "    print(f\"Title: {metadata.title}\")\n",
    "    print(f\"Author: {metadata.author}\")\n",
    "    print(f\"Abstract: {metadata.abstract}\")\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def upload_doc(file_path, file_search_store):\n",
    "    \"\"\"Upload a document to the file search store\"\"\"\n",
    "\n",
    "    file_name = os.path.basename(file_path)\n",
    "\n",
    "    print(f\"Uploading {file_name} for metadata extraction...\")\n",
    "    temp_file = client.files.upload(file=file_path)\n",
    "\n",
    "    # Verify file is active (ready for inference)\n",
    "    while temp_file.state.name == \"PROCESSING\":\n",
    "        print(\"Still uploading...\", end=\"\\r\")\n",
    "        time.sleep(2)\n",
    "        temp_file = client.files.get(name=temp_file.name)\n",
    "\n",
    "    if temp_file.state.name != \"ACTIVE\":\n",
    "        raise RuntimeError(f\"File upload failed with state: {temp_file.state.name}\")\n",
    "\n",
    "    # Now let's check if this is a replacement of an existing file\n",
    "    # If so, we should delete the existing entry first\n",
    "    # Iterate through all docs in the store\n",
    "    for doc in client.file_search_stores.documents.list(parent=file_search_store.name):\n",
    "        should_delete = False\n",
    "\n",
    "        # Match by Display Name\n",
    "        if doc.display_name == file_name:\n",
    "            should_delete = True\n",
    "\n",
    "        # Match by Custom Metadata\n",
    "        # This catches docs where display_name was set to the Title\n",
    "        elif doc.custom_metadata:\n",
    "            for meta in doc.custom_metadata:\n",
    "                if meta.key == \"file_name\" and meta.string_value == file_name:\n",
    "                    should_delete = True\n",
    "                    break\n",
    "\n",
    "        if should_delete:\n",
    "            delete_doc(doc)\n",
    "\n",
    "    metadata = generate_metadata(file_name, temp_file)\n",
    "\n",
    "    # Import the file into the file search store with custom metadata\n",
    "    operation = client.file_search_stores.upload_to_file_search_store(\n",
    "        file_search_store_name=file_search_store.name,\n",
    "        file=file_path,\n",
    "        config={\n",
    "            \"display_name\": metadata.title,  # or we could determine the title\n",
    "            # 'chunking_config' : chunking_config[\"chunking_config\"],\n",
    "            \"custom_metadata\": [\n",
    "                {\"key\": \"title\", \"string_value\": metadata.title},\n",
    "                {\"key\": \"file_name\", \"string_value\": file_name},\n",
    "                {\"key\": \"author\", \"string_value\": metadata.author},\n",
    "                {\"key\": \"abstract\", \"string_value\": metadata.abstract},\n",
    "            ],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Wait until import is complete\n",
    "    while not operation.done:\n",
    "        time.sleep(5)\n",
    "        print(\"Still importing...\")\n",
    "        operation = client.operations.get(operation)\n",
    "\n",
    "    print(f\"{file_name} successfully uploaded and indexed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdd1486",
   "metadata": {},
   "source": [
    "Now actually **upload and process our documents**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb63221",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_search_store = get_store(STORE_NAME)\n",
    "if file_search_store is None:\n",
    "    print(f\"Store {STORE_NAME} not found.\")\n",
    "else:\n",
    "    print(f\"Uploading files to {file_search_store.name}...\")\n",
    "    files_to_upload = glob.glob(f\"{UPLOAD_PATH}/*\")\n",
    "    if files_to_upload:\n",
    "        for file_path in files_to_upload:\n",
    "            print(f\"Uploading {file_path}\")\n",
    "            upload_doc(file_path, file_search_store)\n",
    "        print(\"Upload complete.\")\n",
    "    else:\n",
    "        print(f\"No files found in {UPLOAD_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2c6c80",
   "metadata": {},
   "source": [
    "## Verify with Query\n",
    "\n",
    "Now that the data is uploaded, let's verify we can retrieve it using the File Search Tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc16c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the store again to be sure\n",
    "store = get_store(STORE_NAME)\n",
    "question = \"\"\"Give me a brief 4 step plan to optimise migration to cloud, \n",
    "achieving the fastest ROI and lowest overall TCO\"\"\"\n",
    "\n",
    "if store:\n",
    "    print(f\"Querying store: {store.name} ({store.display_name})\")\n",
    "\n",
    "    try:\n",
    "        # Use the File Search Tool\n",
    "        if hasattr(types, \"FileSearch\"):\n",
    "            print(\"FileSearch tool config...\")\n",
    "            response = client.models.generate_content(\n",
    "                model=MODEL,\n",
    "                contents=question,\n",
    "                config=types.GenerateContentConfig(\n",
    "                    tools=[types.Tool(file_search=types.FileSearch(file_search_store_names=[store.name]))]\n",
    "                ),\n",
    "            )\n",
    "            print(\"\\nResponse:\")\n",
    "            print(response.text)\n",
    "        else:\n",
    "            print(\"types.FileSearch not found. Skipping in-notebook query verification.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Query failed: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"Store not found, cannot verify.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61c51b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
