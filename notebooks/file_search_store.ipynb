{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93b1ec65",
   "metadata": {},
   "source": [
    "# File Search Store Management for Rickbot\n",
    "\n",
    "A notebook to experiment with the FileSearchStore and how it can be used to manage file search in the Rickbot Agent.\n",
    "\n",
    "The best way to run this notebook is from Google Colab.\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/derailed-dash/rickbot-adk/blob/main/notebooks/file_search_store.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=30/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a639f75a",
   "metadata": {},
   "source": [
    "## Pre-Reqs and Notes\n",
    "\n",
    "- The `file_search_stores` is a feature exclusive to the Gemini Developer API. \n",
    "  - It does not work with the Vertex AI API or the Gen AI SDK in Vertex AI mode.\n",
    "  - Therefore: don't set env vars for `GOOGLE_CLOUD_LOCATION` or `GOOGLE_GENAI_USE_VERTEXAI` and do not initialise Vertex AI.\n",
    "- Make sure you have an up-to-date version of the `google-genai` package installed. \n",
    "  - Versions older than 1.49.0 do not support the File Search Tool.\n",
    "  - You can upgrade all packages using `uv sync --upgrade`.\n",
    "  - Or just `google-genai` using `uv sync --upgrade-package google-genai`\n",
    "  - Or, if using `pip`: `pip install --upgrade google-genai`.\n",
    "  - You can add to your `pyproject.toml` file; since we don't explicitly need it outside \n",
    "- Add your Gemini API Key to Colab as a secret. Then you can retrieve it using `userdata.get(\"GEMINI_API_KEY\")`\n",
    "\n",
    "**Remember that your File Search Store is linked to your project, and this is linked to your GEMINI_API_KEY.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d68e9c1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97210f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e576bcec",
   "metadata": {},
   "source": [
    "\n",
    "### Local Only\n",
    "\n",
    "If running locally, setup the Google Cloud environment:\n",
    "\n",
    "```bash\n",
    "source scripts/setup-env.sh\n",
    "```\n",
    "\n",
    "Then to install the package dependencies into the virtual environment, use the `uv` tool:\n",
    "\n",
    "1. From your agent's root directory, run `make install` to set up the virtual environment (`.venv`).\n",
    "2. In this Jupyter notebook, select the kernel from the `.venv` folder to ensure all dependencies are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a1ade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load env vars\n",
    "if load_dotenv(override=True):\n",
    "    print(\"Successfully loaded environment variables.\")\n",
    "else:\n",
    "    print(\"Failed to load environment variables.\")\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not GEMINI_API_KEY:\n",
    "    raise ValueError(\"GEMINI_API_KEY environment variable not set.\")\n",
    "else:\n",
    "    print(f\"Successfully loaded Gemini API key, ending {GEMINI_API_KEY[-3:]}\")\n",
    "\n",
    "MODEL = os.getenv(\"MODEL\")\n",
    "if not MODEL:\n",
    "    print(\"Warning: MODEL environment variable not set.\")\n",
    "else:\n",
    "    print(f\"Successfully loaded model: {MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9a6f7f",
   "metadata": {},
   "source": [
    "### Or In Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ee8b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -U \"google-genai>=1.49.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bb10c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "\n",
    "os.environ[\"GEMINI_API_KEY\"] = userdata.get(\"GEMINI_API_KEY\")\n",
    "os.environ[\"MODEL\"] = userdata.get(\"MODEL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a074a3",
   "metadata": {},
   "source": [
    "### Client Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5175e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client()\n",
    "STORE_NAME = \"rickbot-dazbo-ref\" # as per personalities.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42055dd4",
   "metadata": {},
   "source": [
    "## Store Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c9c175",
   "metadata": {},
   "source": [
    "### View All Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for a_store in client.file_search_stores.list():\n",
    "        print(a_store)\n",
    "except Exception as e:\n",
    "    print(f\"Error listing stores (check creds?): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f0dfbd",
   "metadata": {},
   "source": [
    "### Retrieve the Store\n",
    "\n",
    "Here's a utility function to retrieve the store(s) that match a given display name. Note that display name is not unique, so this function returns the first matching store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b80190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_store(store_name: str):\n",
    "    \"\"\"Retrieve a store by its display name\"\"\"\n",
    "    try:\n",
    "        for a_store in client.file_search_stores.list():\n",
    "            if a_store.display_name == store_name:\n",
    "                return a_store\n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_store path: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d3346a",
   "metadata": {},
   "source": [
    "### Create the Store (One Time)\n",
    "\n",
    "Once you've created the store, save the store ID for use in your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6954029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not get_store(STORE_NAME):\n",
    "    file_search_store = client.file_search_stores.create(config={\"display_name\": STORE_NAME})\n",
    "    print(f\"Created store: {file_search_store.name}\")\n",
    "else:\n",
    "    print(f\"Store {STORE_NAME} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebe601a",
   "metadata": {},
   "source": [
    "### View the Store\n",
    "\n",
    "We can interrogate a store and see what files have been uploaded to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc694cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_search_store = get_store(STORE_NAME)\n",
    "if not file_search_store:\n",
    "    print(f\"Store {STORE_NAME} not found.\")\n",
    "else:\n",
    "    print(file_search_store)\n",
    "\n",
    "    # List all documents in the store\n",
    "    # The 'parent' argument is the resource name of the store\n",
    "    docs = client.file_search_stores.documents.list(parent=file_search_store.name)\n",
    "    try:\n",
    "        doc_list = list(docs)\n",
    "        print(f\"Docs in {STORE_NAME}: {len(doc_list)}\")\n",
    "\n",
    "        if not doc_list:\n",
    "            print(\"No documents found in the store.\")\n",
    "        else:\n",
    "            for i, doc in enumerate(doc_list):\n",
    "                section_heading = f\"Document {i}:\"\n",
    "                print(\"-\" * len(section_heading))\n",
    "                print(section_heading)\n",
    "                print(\"-\" * len(section_heading))\n",
    "                print(f\"  Display name:{doc.display_name}\")\n",
    "                print(f\"  ID: {doc.name}\")\n",
    "                print(f\"  Metadata: {doc.custom_metadata}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing docs (might be empty): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c82077a",
   "metadata": {},
   "source": [
    "### Delete Store(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d6987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, point to the right store. For example:\n",
    "store_to_delete = get_store(STORE_NAME)\n",
    "\n",
    "# Delete the store\n",
    "if store_to_delete:\n",
    "    print(f\"Deleting: {store_to_delete.name}\")\n",
    "    # Uncomment to delete\n",
    "    # client.file_search_stores.delete(name=store_to_delete.name, config={'force': True})\n",
    "else:\n",
    "    print(\"Store not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11668f60",
   "metadata": {},
   "source": [
    "## Upload and Process Files\n",
    "\n",
    "Now we need to place the files in a suitable local folder to upload to the store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad4e1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPLOAD_PATH = \"/content/upload-files/\"\n",
    "UPLOAD_PATH = \"../scratch/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b5a9d7",
   "metadata": {},
   "source": [
    "Create some utility classes and functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6f54dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentMetadata(BaseModel):\n",
    "    \"\"\"Metadata for a document\"\"\"    \n",
    "    title: str\n",
    "    author: str\n",
    "    abstract: str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2312c1d3",
   "metadata": {},
   "source": [
    "#### Async Implementation\n",
    "\n",
    "Processing multiple input documents sequentially is slow. So let's add a multithreaded approach..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f012d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_delete_doc(doc):\n",
    "    \"\"\"Async delete document from its file search store.\"\"\"\n",
    "    print(f\"‚ôªÔ∏è DELETING DUPLICATE: '{doc.display_name}' (ID: {doc.name})\")\n",
    "    await client.aio.file_search_stores.documents.delete(name=doc.name, config={\"force\": True})\n",
    "    await asyncio.sleep(1)\n",
    "\n",
    "async def async_generate_metadata(file_name: str, temp_file) -> DocumentMetadata:\n",
    "    \"\"\"Async generate metadata for a document.\"\"\"\n",
    "    print(f\"EXTRACTING METADATA FROM {file_name}...\")\n",
    "    response = await client.aio.models.generate_content(\n",
    "        model=MODEL,\n",
    "        contents=[\n",
    "            \"\"\"Please extract title, author, and short abstract from this document. \n",
    "            Each value should be under 200 characters.\n",
    "\n",
    "            Abstracts should be succinct and NOT include preamble text like `This document describes...`\n",
    "\n",
    "            Example bad abstract: \n",
    "            Now I want to cover a key consideration that can potentially \n",
    "            save you more in future IT spend than any other decision you can make: \n",
    "            embracing open source as a core element of your cloud strategy.\n",
    "\n",
    "            Example good abstract:\n",
    "            How you can significantly reduce IT spend by embracing open source\n",
    "            as a core component of your cloud strategy.\n",
    "\n",
    "            Example bad abstract:\n",
    "            This article discusses how you can design your cloud landing zone.\n",
    "\n",
    "            Example good abstract:\n",
    "            How to design your cloud landing zone according to best practices.\n",
    "            \"\"\",\n",
    "            temp_file,\n",
    "        ],\n",
    "        config={\"response_mime_type\": \"application/json\", \"response_schema\": DocumentMetadata},\n",
    "    )\n",
    "    metadata: DocumentMetadata = response.parsed\n",
    "    print(f\"Title: {metadata.title}\")\n",
    "    print(f\"Author: {metadata.author}\")\n",
    "    print(f\"Abstract: {metadata.abstract}\")\n",
    "\n",
    "    return metadata\n",
    "\n",
    "async def async_upload_doc(file_path, file_search_store, semaphore, last_modified):\n",
    "    \"\"\"Simple async upload with last_modified metadata.\"\"\"\n",
    "    async with semaphore:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        temp_file = await client.aio.files.upload(file=file_path)\n",
    "\n",
    "        while temp_file.state.name == \"PROCESSING\":\n",
    "            await asyncio.sleep(2)\n",
    "            temp_file = await client.aio.files.get(name=temp_file.name)\n",
    "\n",
    "        # This is the expensive part we want to skip if possible!\n",
    "        metadata = await async_generate_metadata(file_name, temp_file)\n",
    "\n",
    "        operation = await client.aio.file_search_stores.upload_to_file_search_store(\n",
    "            file_search_store_name=file_search_store.name,\n",
    "            file=file_path,\n",
    "            config={\n",
    "                \"display_name\": metadata.title,\n",
    "                \"custom_metadata\": [\n",
    "                    {\"key\": \"title\", \"string_value\": metadata.title},\n",
    "                    {\"key\": \"file_name\", \"string_value\": file_name},\n",
    "                    {\"key\": \"author\", \"string_value\": metadata.author},\n",
    "                    {\"key\": \"abstract\", \"string_value\": metadata.abstract},\n",
    "                    {\"key\": \"last_modified\", \"string_value\": str(last_modified)},\n",
    "                ],\n",
    "            },\n",
    "        )\n",
    "        while not operation.done:\n",
    "            await asyncio.sleep(5)\n",
    "            operation = await client.aio.operations.get(operation)\n",
    "        print(f\"‚úÖ {file_name} UPLOADED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdd1486",
   "metadata": {},
   "source": [
    "Now actually **upload and process our documents**.\n",
    "\n",
    "We'll use an async method to process docs in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d67815",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def robust_batch_upload(upload_path, store_name, batch_size=10):\n",
    "    store = get_store(store_name)\n",
    "    files = glob.glob(f\"{upload_path}/*\")\n",
    "    if not files:\n",
    "        print(\"No files found.\")\n",
    "        return\n",
    "\n",
    "    # Phase 1: Scan Store and Evaluate\n",
    "    print(f\"üîç Scanning store {store.display_name} for existing files...\")\n",
    "    existing_docs = {}\n",
    "    async for doc in await client.aio.file_search_stores.documents.list(parent=store.name):\n",
    "        # Extract the original file name from metadata\n",
    "        fname = next((m.string_value for m in doc.custom_metadata if m.key == 'file_name'), None)\n",
    "        if fname:\n",
    "            existing_docs.setdefault(fname, []).append(doc)\n",
    "\n",
    "    delete_tasks = []\n",
    "    files_to_upload = []\n",
    "\n",
    "    for fp in files:\n",
    "        fname = os.path.basename(fp)\n",
    "        local_mtime = os.path.getmtime(fp)\n",
    "\n",
    "        should_upload = True\n",
    "        if fname in existing_docs:\n",
    "            # Check if any existing version is up-to-date\n",
    "            up_to_date = False\n",
    "            for doc in existing_docs[fname]:\n",
    "                stored_mtime_str = next((m.string_value for m in doc.custom_metadata if m.key == 'last_modified'), \"0\")\n",
    "                try:\n",
    "                    stored_mtime = float(stored_mtime_str)\n",
    "                except ValueError:\n",
    "                    stored_mtime = 0\n",
    "\n",
    "                # If we have a version that is as new or newer, we don't need to do anything\n",
    "                if stored_mtime >= local_mtime:\n",
    "                    up_to_date = True\n",
    "                    break\n",
    "\n",
    "            if up_to_date:\n",
    "                print(f\"‚è≠Ô∏è  SKIPPING: '{fname}' is already up-to-date.\")\n",
    "                should_upload = False\n",
    "            else:\n",
    "                # Local version is newer (or no timestamp found), mark existing for deletion\n",
    "                for doc in existing_docs[fname]:\n",
    "                    delete_tasks.append(async_delete_doc(doc))\n",
    "\n",
    "        if should_upload:\n",
    "            files_to_upload.append((fp, local_mtime))\n",
    "\n",
    "    # Phase 2: Cleanup Outdated Docs\n",
    "    if delete_tasks:\n",
    "        print(f\"üóëÔ∏è  Deleting {len(delete_tasks)} outdated documents...\")\n",
    "        await asyncio.gather(*delete_tasks)\n",
    "\n",
    "    # Phase 3: Parallel Upload New/Updated Files\n",
    "    if files_to_upload:\n",
    "        print(f\"üöÄ Starting parallel upload of {len(files_to_upload)} files...\")\n",
    "        semaphore = asyncio.Semaphore(batch_size)\n",
    "        upload_tasks = [async_upload_doc(fp, store, semaphore, mtime) for fp, mtime in files_to_upload]\n",
    "        await asyncio.gather(*upload_tasks)\n",
    "        print(\"üéâ Robust batch upload complete.\")\n",
    "    else:\n",
    "        print(\"‚úÖ All files are already up-to-date in the filestore.\")\n",
    "\n",
    "await robust_batch_upload(UPLOAD_PATH, STORE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2c6c80",
   "metadata": {},
   "source": [
    "## Verify with Query\n",
    "\n",
    "Now that the data is uploaded, let's verify we can retrieve it using the File Search Tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc16c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the store again to be sure\n",
    "store = get_store(STORE_NAME)\n",
    "question = \"\"\"Give me a brief 4 step plan to optimise migration to cloud, \n",
    "achieving the fastest ROI and lowest overall TCO\"\"\"\n",
    "\n",
    "if store:\n",
    "    print(f\"Querying store: {store.name} ({store.display_name})\")\n",
    "\n",
    "    try:\n",
    "        # Use the File Search Tool\n",
    "        if hasattr(types, \"FileSearch\"):\n",
    "            print(\"FileSearch tool config...\")\n",
    "            response = client.models.generate_content(\n",
    "                model=MODEL,\n",
    "                contents=question,\n",
    "                config=types.GenerateContentConfig(\n",
    "                    tools=[types.Tool(file_search=types.FileSearch(file_search_store_names=[store.name]))]\n",
    "                ),\n",
    "            )\n",
    "            print(\"\\nResponse:\")\n",
    "            print(response.text)\n",
    "        else:\n",
    "            print(\"types.FileSearch not found. Skipping in-notebook query verification.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Query failed: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"Store not found, cannot verify.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61c51b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
